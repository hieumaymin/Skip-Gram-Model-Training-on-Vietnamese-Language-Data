{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc và tiền xử lý dữ liệu\n",
    "def preprocess_text(text):\n",
    "    # Chuyển chữ thường\n",
    "    text = text.lower()\n",
    "    # Loại bỏ ký tự số\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Loại bỏ các ký tự đặc biệt và dấu câu\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tách từ tiếng Việt\n",
    "    tokenized_text = ViTokenizer.tokenize(text)\n",
    "    return tokenized_text.split()\n",
    "\n",
    "# Tạo từ điển từ vựng\n",
    "def build_vocab(corpus):\n",
    "    word_counts = Counter(corpus)\n",
    "    # Sắp xếp từ vựng theo thứ tự chữ cái\n",
    "    sorted_vocab = sorted(word_counts.items())\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(sorted_vocab)}\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    return vocab, idx_to_word\n",
    "\n",
    "# Tạo cặp từ trung tâm-ngữ cảnh\n",
    "def generate_skipgram_pairs(corpus, vocab, window_size=1):\n",
    "    pairs = []\n",
    "    for i, word in enumerate(corpus):\n",
    "        center_word_idx = vocab[word]\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j == 0 or i + j < 0 or i + j >= len(corpus):\n",
    "                continue\n",
    "            context_word_idx = vocab[corpus[i + j]]\n",
    "            pairs.append((center_word_idx, context_word_idx))\n",
    "    return pairs\n",
    "\n",
    "# Đọc và tiền xử lý dữ liệu từ file\n",
    "with open('./data/data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "corpus = preprocess_text(text)\n",
    "vocab, idx_to_word = build_vocab(corpus)\n",
    "pairs = generate_skipgram_pairs(corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu từ vựng vào file\n",
    "with open('./data/vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for word, idx in vocab.items():\n",
    "        f.write(f\"{word}\\t{idx}\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embed_size, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Khởi tạo trọng số\n",
    "        self.W1 = np.random.rand(vocab_size, embed_size) \n",
    "        self.W2 = np.random.rand(embed_size, vocab_size) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def forward(self, center_word_idx):\n",
    "        # Lớp nhúng (W1)\n",
    "        h = self.W1[center_word_idx]\n",
    "        # Lớp đầu ra (W2)\n",
    "        u = np.dot(h, self.W2)\n",
    "        y_pred = self.softmax(u)\n",
    "        return y_pred, h\n",
    "\n",
    "    def backward(self, center_word_idx, context_word_idx, y_pred, h):\n",
    "        error = y_pred\n",
    "        error[context_word_idx] -= 1  # Cập nhật lỗi cho từ ngữ cảnh\n",
    "\n",
    "        # Gradient cho W2 và W1\n",
    "        dW2 = np.outer(h, error)\n",
    "        dW1 = np.dot(error, self.W2.T)\n",
    "\n",
    "        # Cập nhật trọng số\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.W1[center_word_idx] -= self.lr * dW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, pairs, epochs=100, batch_size=64):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        # Chia pairs thành các batch\n",
    "        num_batches = len(pairs) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch = pairs[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            batch_loss = 0\n",
    "            for center_word_idx, context_word_idx in batch:\n",
    "                y_pred, h = model.forward(center_word_idx)\n",
    "                #cross-entrophy\n",
    "                loss = -np.log(y_pred[context_word_idx]) \n",
    "                batch_loss += loss\n",
    "                model.backward(center_word_idx, context_word_idx, y_pred, h)\n",
    "            total_loss += batch_loss / len(batch)  # Tính loss trung bình của batch\n",
    "        losses.append(total_loss / num_batches)  # Tính loss trung bình của tất cả các batch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {losses[-1]}\")\n",
    "    return losses\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "embed_size = 100\n",
    "skipgram_model = SkipGram(len(vocab), embed_size, learning_rate=0.01)\n",
    "\n",
    "# Huấn luyện\n",
    "losses = train_skipgram(skipgram_model, pairs, epochs=50, batch_size=128) \n",
    "\n",
    "# Vẽ đồ thị mất mát\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính toán cosine similarity giữa hai vector\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Cặp đồng nghĩa: (\"đầu_tiên\", \"ban_đầu\"),(\"bố\", \"cha\")\n",
    "# Cặp trái nghĩa: (\"lên\", \"xuống\"), (\"lớn\", \"nhỏ\")\n",
    "\n",
    "# Kiểm tra tương đồng cosine cho các cặp từ\n",
    "word_pairs = [(\"lên\", \"xuống\"), (\"lớn\", \"nhỏ\"), (\"đầu_tiên\", \"ban_đầu\"),(\"bố\", \"cha\")]\n",
    "for word1, word2 in word_pairs:\n",
    "    idx1 = vocab[word1]\n",
    "    idx2 = vocab[word2]\n",
    "    vec1 = skipgram_model.W1[idx1]\n",
    "    vec2 = skipgram_model.W1[idx2]\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "\n",
    "# Trực quan hóa vector nhúng bằng PCA cho các từ trong word_pairs\n",
    "def visualize_word_pairs(skipgram_model, word_pairs, vocab, n_components=2):\n",
    "    embeddings = []\n",
    "    words = []\n",
    "\n",
    "    # Lấy vector nhúng của các từ trong word_pairs\n",
    "    for word1, word2 in word_pairs:\n",
    "        idx1 = vocab[word1]\n",
    "        idx2 = vocab[word2]\n",
    "        embeddings.append(skipgram_model.W1[idx1])\n",
    "        embeddings.append(skipgram_model.W1[idx2])\n",
    "        words.append(word1)\n",
    "        words.append(word2)\n",
    "\n",
    "    # Áp dụng PCA để giảm chiều\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(np.array(embeddings))\n",
    "\n",
    "    # Vẽ đồ thị trực quan hóa PCA\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, (x, y) in enumerate(reduced_embeddings):\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.01, y + 0.01, words[i], fontsize=12)\n",
    "\n",
    "    plt.title(\"PCA Visualization of Word Pairs\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Gọi hàm trực quan hóa cho các cặp từ\n",
    "visualize_word_pairs(skipgram_model, word_pairs, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
